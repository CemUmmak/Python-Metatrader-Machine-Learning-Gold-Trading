from ML_Ind_V3.Settings import *

import joblib
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

# ğŸ“‚ 1. Inputs
data_path = path_csv_cleaned_nolast5000
model_no = 888

# 2. Read Data
data_name = get_data_name_from_path(data_path)

df = pd.read_csv(data_path)
# df = create_lag_features(df, n_lags=20, label_column="ProfitLabel")

os.makedirs(os.path.join("Models", data_name), exist_ok=True)

print(f"âœ… Toplam satÄ±r sayÄ±sÄ± : {df.shape[0]}")
print(f"âœ… Toplam feature sayÄ±sÄ± : {df.shape[1]}")

df.drop(columns=["Time"], inplace=True)

X = df.drop("ProfitLabel", axis=1)
y = df["ProfitLabel"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""custom_params = {
    "n_estimators": 300,          # Daha fazla aÄŸaÃ§ â†’ daha istikrarlÄ± sonuÃ§lar
    "criterion": "gini",          # Gini impurity
    "max_depth": 6,               # AÄŸaÃ§larÄ±n maksimum derinliÄŸi: overfitting engeller
    "min_samples_split": 5,       # Bir dÃ¼ÄŸÃ¼mÃ¼ bÃ¶lmek iÃ§in minimum 5 Ã¶rnek olsun
    "min_samples_leaf": 3,        # Yaprakta minimum 3 Ã¶rnek â†’ overfit Ã¶nler
    "min_weight_fraction_leaf": 0.0,
    "max_features": "sqrt",       # Default zaten bu â†’ her split'te karekÃ¶k(feature) seÃ§ilir
    "max_leaf_nodes": None,       # BÄ±rakÄ±yoruz, gerekirse sonra kÄ±sÄ±tlarÄ±z
    "bootstrap": True,            # Bootstrap aÃ§Ä±k â†’ Ã§eÅŸitlilik saÄŸlar
    "oob_score": False,           # Åu anda oob kapalÄ±, dilersen aÃ§arÄ±z
    "n_jobs": -1,                 # Full CPU kullanÄ±mÄ±
    "random_state": 42,           # Sabit random
    "verbose": 0,
    "class_weight": "balanced_subsample"  # âš¡ Class imbalance iÃ§in kÃ¼Ã§Ã¼k ayar (sample iÃ§i dengeleme)
}"""

custom_params = {
    "n_estimators": 1200,              # Ã‡ok sayÄ±da aÄŸaÃ§ â†’ istikrar ve dÃ¼ÅŸÃ¼k varyans
    "criterion": "entropy",            # Bilgi kazancÄ± odaklÄ±, daha keskin kararlar
    "max_depth": 18,                   # Derinlik yÃ¼ksek â†’ detaylÄ± Ã¶ÄŸrenme
    "min_samples_split": 10,           # Daha bÃ¼yÃ¼k alt kÃ¼meler â†’ emin olmadan bÃ¶lme
    "min_samples_leaf": 5,             # Yaprakta en az 5 Ã¶rnek â†’ noise azaltma
    "max_features": 0.3,               # Her splitâ€™te daha az feature bak â†’ Ã§eÅŸitlilik
    "bootstrap": True,
    "oob_score": True,
    "n_jobs": -1,
    "random_state": 42,
    "class_weight": "balanced_subsample",
    "verbose": 0
}


model = RandomForestClassifier(**custom_params)

# ğŸ‹ï¸ 6. Modeli eÄŸit
model.fit(X_train_scaled, y_train)

# ğŸ”® 7. Test verisi Ã¼zerinde tahmin yap
y_pred = model.predict(X_test_scaled)

# ğŸ“ˆ 8. Performans metriklerini hesapla
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average="macro")
recall = recall_score(y_test, y_pred, average="macro")
f1 = f1_score(y_test, y_pred, average="macro")

print("\n=== Model Evaluation Metrics ===")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

print("\nğŸ¯ Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

print("\nğŸ“Š Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

joblib.dump(model, f"Models/{data_name}/{data_name}_RF_Model_{model_no}.pkl")
joblib.dump(scaler, f"Models/{data_name}/{data_name}_RF_Scaler_{model_no}.pkl")

# ğŸ’¾ 10. Model parametrelerini CSV olarak kaydet
df_params = pd.DataFrame([custom_params])
df_params.to_csv(f"Models/{data_name}/{data_name}_RF_Params_{model_no}.csv", index=False)

print(f"\nâœ… Model, Scaler ve Parametreler 'Models' klasÃ¶rÃ¼ne kaydedildi.")
